{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eed2298-5a5e-44bf-bf2c-5b6b48df2839",
   "metadata": {},
   "source": [
    "### M P I = Message Passing Interface\n",
    "\n",
    "MPI is a **specification** for the developers and users of __message passing libraries__. By itself, it is **NOT a library** - but rather the specification of what such a library should be. It specifies the names, calling sequences, and results of subroutines to be called from Fortran programs and the functions to be called from C programs. The programs that users write in Fortran and C are compiled with ordinary compilers and linked with the MPI library.\n",
    "\n",
    "MPI primarily addresses the message-passing parallel programming model: data is moved from the address space of one process to that of another process through cooperative operations on each process.\n",
    "\n",
    "\r\n",
    "MPI is a specification, not a particular implementation. As of this writing, all parallel computer vendors offer an MPI implementation for their machines and free, publicly available implementations can be downloaded over the Internet. A correct MPI program should be able to run on all MPI implementations without change. \r\n",
    "\n",
    "\n",
    "<img src=\"./images/mpi-p2p.png\" alt=\"Send and Receive Model\" style=\"height: 200px; width:500px;\"/>\n",
    "\n",
    "Simply stated, the goal of the Message Passing Interface is to provide a widely used standard for writing message passing programs. The interface attempts to be:\n",
    "- Practical\n",
    "- Portable\n",
    "- Efficient\n",
    "- Flexible\n",
    "The MPI standard has gone through a number of revisions, with the most recent version b\n",
    "\n",
    "### Programming Model\n",
    "Originally, MPI was designed for distributed memory architectures, which were becoming increasingly popular at that time (1980s - early 1990s).\n",
    "\n",
    "Distributed Memory\n",
    "\n",
    "<img src=\"./images/distributed_mem.gif\" alt=\"Distributed Memory Model\" style=\"height: 200px; width:500px;\"/>\n",
    "\n",
    "As architecture trends changed, shared memory SMPs were combined over networks creating hybrid distributed memory / shared memory systems.\n",
    "<img src=\"./images/hybrid_mem.gif\" alt=\"Hybrid Memory Model\" style=\"height: 200px; width:500px;\"/>\n",
    "\n",
    "MPI implementors adapted their libraries to handle both types of underlying memory architectures seamlessly. They also adapted/developed ways of handling different interconnects and protocols.\n",
    "\n",
    "Hybrid Memoryeing MPI-3.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38a34b19-6d9e-49ff-bd0c-4d9e413595af",
   "metadata": {},
   "source": [
    "# The Message-Passing Model\n",
    "\n",
    "- A process is (traditionally) a program counter and address space.\r",
    "- \n",
    "Processes may have multiple threads (program counters and associated stacks) sharing a single address space.  MPI is for communication among processes, which have separate address spaces.- \r\n",
    "Interprocess communication consists of-  \r\n",
    "Synchronizati- on\r\n",
    "Movement of data from one process’s address space to anoth\n",
    "## Cooperative Operations for Communication\n",
    "The message-passing approach makes the exchange of data cooperative.\r\n",
    "Data is explicitly sent by one process and received by another.\r\n",
    "An advantage is that any change in the receiving process’s memory is made with the receiver’s explicit participatio\n",
    "\n",
    "### One-Sided Operations for Communicationn.\r\n",
    "Communication and synchronization are combi\n",
    "\n",
    "- One-sided operations between processes include remote memory reads and writes\r",
    "- \n",
    "Only one process needs to explicitly participate.- \r\n",
    "An advantage is that communication and synchronization are decouple- d\r\n",
    "One-sided operations are part of MPI-2.ed.\r\n",
    "er’s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577cd48-490b-4308-8955-2a955e68be51",
   "metadata": {},
   "source": [
    "## MPI concepts\n",
    "\n",
    "MPI provides several features. The following concepts provide context for all of those abilities and help the programmer to decide what functionality to use in their application programs. Four of MPI's eight basic concepts are unique to MPI-2.\n",
    "\n",
    "### Communicator\n",
    "Communicator objects connect groups of processes in the MPI session. Each communicator gives each contained process an independent identifier and arranges its contained processes in an ordered topology. MPI also has explicit groups, but these are mainly good for organizing and reorganizing groups of processes before another communicator is made. MPI understands single group intracommunicator operations, and bilateral intercommunicator communication. In MPI-1, single group operations are most prevalent. Bilateral operations mostly appear in MPI-2 where they include collective communication and dynamic in-process management.\n",
    "\n",
    "Communicators can be partitioned using several MPI commands. These commands include MPI_COMM_SPLIT, where each process joins one of several colored sub-communicators by declaring itself to have that color.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9493c2-9176-4c14-862c-bd42d459949f",
   "metadata": {},
   "source": [
    "\n",
    "### Things that need specifying:\n",
    "- How will “data” be described? \n",
    "- How will processes be identified?\n",
    "- How will the receiver recognize/screen messages?\n",
    "- What will it mean for these operations to complete?\n",
    "\n",
    "# MPI Communication Methods \n",
    "## Point-to-Point Communication \n",
    "MPI Point-to_Point communication is the most used communication method in MPI. It involves the transfer of a message from one process to a particular process in the same communicator. MPI provides blocking (synchronous) and non-blocking (asynchronous) Point-to-Point communication. With blocking communication, an MPI process sends a message to another MPI process and waits until the receiving process completely and correctly receives the message before it continues its work. On the other hand, a sending process using non-blocking communication sends a message to another MPI process and continues its work without waiting to ensure that the message has been correctly received by the receiving process.\n",
    "\n",
    "MPI-1 specifies mechanisms for both blocking and non-blocking point-to-point communication mechanisms, as well as the so-called 'ready-send' mechanism whereby a send request can be made only when the matching receive request has already been made\n",
    "<img src=\"./images/mpi-p2p.png\" alt=\"Send and Receive Model\" style=\"height: 200px; width:500px;\"/>\n",
    "\n",
    "## Collective Communication\n",
    "\n",
    "With this type of MPI communication method, a process broadcasts a message is to all processes in the same communicator including itself.\n",
    "<img src=\"./images/Collective.png\" alt=\"Collective Communications\" style=\"height: 200px; width:500px;\"/>\n",
    "\n",
    "### What is message passing? \n",
    "Process for interchange data that include **Data transfer plus synchronization**\n",
    "\n",
    "## Some Basic Concepts\n",
    "\n",
    "- Processes can be collected into groups.\n",
    "- Each message is sent in a context, and must be received in the same context.\n",
    "- A group and context together form a communicator.\n",
    "- A process is identified by its rank in the group associated with a communicator.\n",
    "- There is a default communicator whose group contains all initial processes, called **MPI_COMM_WORLD**.\n",
    "\n",
    "## MPI Datatypes\n",
    "\n",
    "The data in a message to sent or received is described by a triple (address, count, datatype), where\n",
    "- An MPI datatype is recursively defined as:\n",
    "    - predefined, corresponding to a data type from the language (e.g., **MPI_INT, MPI_DOUBLE_PRECISION**)\n",
    "    - a contiguous array of MPI datatypes\n",
    "    - a strided block of datatypes\n",
    "    - an indexed array of blocks of datatypes\n",
    "    - an arbitrary structure of datatypes\n",
    "There are MPI functions to construct custom datatypes, such an array of (int, float) pairs, or a row of a matrix stored columnwise.\n",
    "\n",
    "## MPI Tags\n",
    "\n",
    "- Messages are sent with an accompanying user-defined integer tag, to assist the receiving process in identifying the message.\n",
    "- Messages can be screened at the receiving end by specifying a specific tag, or not screened by specifying **MPI_ANY_TAG** as the tag in a receive.\n",
    "- Some non-MPI message-passing systems have called tags “message types”. **MPI calls them tags to avoid confusion with datatypes**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d3099-d682-4572-9960-e2ed1d65bba3",
   "metadata": {},
   "source": [
    "### Validate the current env with mpi compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979e99d5-80db-4fc8-8558-62ff90c50501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mpicc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be705e-c8c7-48ee-858d-d800f7b7b548",
   "metadata": {},
   "source": [
    "## We need answer the following question on message passing  env. \n",
    "Two important questions that arise early in a parallel program are:\n",
    "- How many processes are participating in this computation?\n",
    "- Which one am I?\n",
    "MPI provides functions to answer these questions:\n",
    "- **MPI_Comm_size** reports the number of processes.\n",
    "- **MPI_Comm_rank** reports the rank, a number between 0 and size-1, identifying the calling process\n",
    "\n",
    "```c\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/types.h>\n",
    "\n",
    "int main( int argc, char *argv[] )\n",
    "{\n",
    "    int rank, size;\n",
    "    MPI_Init( &argc, &argv );\n",
    "    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n",
    "    MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    "    printf( \"I am %d of %d\\n\", rank, size );\n",
    "    printf(\"My current process id is %ld\", (long)getpid());\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b51cee8f-e206-4d44-902b-0241c80aa950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ./lab1/source/hello_world.c -o hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8910508c-a4b9-4e8a-a86f-21c9659a1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 0 of 3  My current process id is 5338\n",
      "I am 1 of 3  My current process id is 5339\n",
      "I am 2 of 3  My current process id is 5341\n"
     ]
    }
   ],
   "source": [
    "!mpirun -N 3 ./hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4137bf6-30d5-4321-b257-9a76aac35a81",
   "metadata": {},
   "source": [
    "## Communication Types\r\n",
    "\r\n",
    "<img src=\"./images/Blocking-and-non-mpi.png\" alt=\"Send and Receive Model\" style=\"height: 200px; width:500px;\"/>\r\n",
    "\r\n",
    "### MPI Basic (Blocking) Send\r\n",
    "\r\n",
    "**MPI_SEND (start, count, datatype, dest, tag, comm)**\r\n",
    "\r\n",
    "The message buffer is described by (start, count, datatype).\r\n",
    "The target process is specified by dest, which is the rank of the target process in the communicator specified by comm.\r\n",
    "When this function returns, the data has been delivered to the system and the buffer can be reused.  The message may not have been received by the target process.\r\n",
    "\r\n",
    "### MPI Basic (Blocking) Receive\r\n",
    "\r\n",
    "**MPI_RECV(start, count, datatype, source, tag, comm, status)**\r\n",
    "\r\n",
    "Waits until a matching (on source and tag) message is received from the system, and the buffer can be used.\r\n",
    "source is rank in communicator specified by comm, or MPI_ANY_SOURCE.\r\n",
    "status contains further information\r\n",
    "Receiving fewer than count occurrences of datatype is OK, but receiving more is an error.\r\n",
    "\r\n",
    "Sending and receiving are the two foundational concepts of MPI. Almost every single function in MPI can be implemented with basic send and receive calls. In this lesson, I will discuss how to use MPI's blocking sending and receiving functions, and I will also overview other basic concepts associated with transmitting data using MPI.\r\n",
    "\r\n",
    "\r\n",
    "## Overview of sending and receiving with MPI\r\n",
    "MPI's send and receive calls operate in the following manner. First, process *A* decides a message needs to be sent to process *B*. Process A then packs up all of its necessary data into a buffer for process B. These buffers are often referred to as *envelopes* since the data is being packed into a single message before transmission (similar to how letters are packed into envelopes before transmission to the post office). After the data is packed into a buffer, the communication device (which is often a network) is responsible for routing the message to the proper location. The location of the message is defined by the process's rank.\r\n",
    "\r\n",
    "Even though the message is routed to B, process B still has to acknowledge that it wants to receive A's data. Once it does this, the data has been transmitted. Process A is acknowledged that the data has been transmitted and may go back to work.\r\n",
    "\r\n",
    "Sometimes there are cases when A might have to send many different types of messages to B. Instead of B having to go through extra measures to differentiate all these messages, MPI allows senders and receivers to also specify message IDs with the message (known as *tags*). When process B only requests a message with a certain tag number, messages with different tags will be buffered by the network until B is ready for them.\r\n",
    "\r\n",
    "With these concepts in mind, let's look at the prototypes for the MPI sending and receiving functions.\r\n",
    "\r\n",
    "```cpp\r\n",
    "MPI_Send(\r\n",
    "    void* data,\r\n",
    "    int count,\r\n",
    "    MPI_Datatype datatype,\r\n",
    "    int destination,\r\n",
    "    int tag,\r\n",
    "    MPI_Comm communicator)\r\n",
    "```\r\n",
    "\r\n",
    "```cpp\r\n",
    "MPI_Recv(\r\n",
    "    void* data,\r\n",
    "    int count,\r\n",
    "    MPI_Datatype datatype,\r\n",
    "    int source,\r\n",
    "    int tag,\r\n",
    "    MPI_Comm communicator,\r\n",
    "    MPI_Status* status)\r\n",
    "```\r\n",
    "\r\n",
    "Although this might seem like a mouthful when reading all of the arguments, they become easier to remember since almost every MPI call uses similar syntax. The first argument is the data buffer. The second and third arguments describe the count and type of elements that reside in the buffer. `MPI_Send` sends the exact count of elements, and `MPI_Recv` will receive **at most** the count of elements (more on this in the next lesson). The fourth and fifth arguments specify the rank of the sending/receiving process and the tag of the message. The sixth argument specifies the communicator and the last argument (for `MPI_Recv` only) provides information about the received message.\r\n",
    "\r\n",
    "## Elementary MPI datatypes\r\n",
    "The `MPI_Send` and `MPI_Recv` functions utilize MPI Datatypes as a means to specify the structure of a message at a higher level. For example, if the process wishes to send one integer to another, it would use a count of one and a datatype of `MPI_INT`. The other elementary MPI datatypes are listed below with their equivalent C datatypes.\r\n",
    "\r\n",
    "| MPI datatype | C equivalent |\r\n",
    "| --- | --- |\r\n",
    "| MPI_SHORT | short int |\r\n",
    "| MPI_INT | int |\r\n",
    "| MPI_LONG | long int |\r\n",
    "| MPI_LONG_LONG | long long int |\r\n",
    "| MPI_UNSIGNED_CHAR | unsigned char |\r\n",
    "| MPI_UNSIGNED_SHORT | unsigned short int |\r\n",
    "| MPI_UNSIGNED | unsigned int |\r\n",
    "| MPI_UNSIGNED_LONG | unsigned long int |\r\n",
    "| MPI_UNSIGNED_LONG_LONG | unsigned long long int |\r\n",
    "| MPI_FLOAT | float |\r\n",
    "| MPI_DOUBLE | double |\r\n",
    "| MPI_LONG_DOUBLE | long double |\r\n",
    "| MPI_BYTE | char |\r\n",
    "\r\n",
    "For now, we will only make use of these datatypes in the following MPI tutorials in the beginner category.\r\n",
    "\r\n",
    "```\r\n",
    "// Find out rank, size\r\n",
    "int world_rank;\r\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\r\n",
    "int world_size;\r\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &world_size);\r\n",
    "\r\n",
    "int number;\r\n",
    "if (world_rank == 0) {\r\n",
    "    number = -1;\r\n",
    "    MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\r\n",
    "} else if (world_rank == 1) {\r\n",
    "    MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD,\r\n",
    "             MPI_STATUS_IGNORE);\r\n",
    "    printf(\"Process 1 received number %d from process 0\\n\",\r\n",
    "           number);\r\n",
    "}\r\n",
    "```\r\n",
    "MPI_Comm_rank and MPI_Comm_size are first used to determine the world size along with the rank of the process. Then process zero initializes a number to the value of negative one and sends this value to process one. As you can see in the else if statement, process one is calling MPI_Recv to receive the number. It also prints off the received value. Since we are sending and receiving exactly one integer, each process requests that one MPI_INT be sent/received. Each process also uses a tag number of zero to identify the message. The processes could have also used the predefined constant `MPI_Ar -1 from process 0\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9685a56d-60ff-4eda-bd09-b3a24354d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/mnt/c/Users/esher/repositories/mpi_course/lab1/source'\n",
      "mpicc -o send_recv send_recv.c\n",
      "mpicc -o ping_pong ping_pong.c\n",
      "mpicc -o ring ring.c\n",
      "make: Leaving directory '/mnt/c/Users/esher/repositories/mpi_course/lab1/source'\n"
     ]
    }
   ],
   "source": [
    "!make all -C ./lab1/source/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629adf0-7981-4658-8792-417a745c37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -N 2 ./lab1/source/ping_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65f1b78d-7a87-4033-9e57-faf8f89c695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: srun: command not found\n"
     ]
    }
   ],
   "source": [
    "!srun -N 2 ./lab1/source/ping_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8802a28-6f7e-467a-842c-c2700387afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!srun -N 2 ./lab1/source/send_recv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
